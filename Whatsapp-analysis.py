# -*- coding: utf-8 -*-
"""NLP_PROJ_WP_analyse.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TBzh60QRWC0nPnm2bqNG12Ui0IwFN2A8
"""


# Commented out IPython magic to ensure Python compatibility.
import re
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import emoji
import itertools
from collections import Counter
import warnings

# %matplotlib inline
warnings.filterwarnings('ignore')

def rawToDf(file, key):
    '''Converts raw .txt file into a Data Frame'''

    split_formats = {
        '12hr' : '\d{1,2}/\d{1,2}/\d{2,4},\s\d{1,2}:\d{2}\s[APap][mM]\s-\s',
        '24hr' : '\d{1,2}/\d{1,2}/\d{2,4},\s\d{1,2}:\d{2}\s-\s',
        'custom' : ''
    }
    datetime_formats = {
        '12hr' : '%d/%m/%Y, %I:%M %p - ',
        '24hr' : '%d/%m/%Y, %H:%M - ',
        'custom': ''
    }

    with open(file, 'r', encoding='utf-8') as raw_data:
        # print(raw_data.read())
        raw_string = ' '.join(raw_data.read().split('\n')) # converting the list split by newline char. as one whole string as there can be multi-line messages
        user_msg = re.split(split_formats[key], raw_string) [1:] # splits at all the date-time pattern, resulting in list of all the messages with user names
        date_time = re.findall(split_formats[key], raw_string) # finds all the date-time patterns

        df = pd.DataFrame({'date_time': date_time, 'user_msg': user_msg}) # exporting it to a df

    # converting date-time pattern which is of type String to type datetime,
    # format is to be specified for the whole string where the placeholders are extracted by the method
    df['date_time'] = pd.to_datetime(df['date_time'], format=datetime_formats[key])

    # split user and msg
    usernames = []
    msgs = []
    for i in df['user_msg']:
        a = re.split('([\w\W]+?):\s', i) # lazy pattern match to first {user_name}: pattern and spliting it aka each msg from a user
        if(a[1:]): # user typed messages
            usernames.append(a[1])
            msgs.append(a[2])
        else: # other notifications in the group(eg: someone was added, some left ...)
            usernames.append("group_notification")
            msgs.append(a[0])

    # creating new columns
    df['user'] = usernames
    df['message'] = msgs

    # dropping the old user_msg col.
    df.drop('user_msg', axis=1, inplace=True)

    return df

df = rawToDf('/content/drive/MyDrive/NLP_PROJ/whatsapp-chat-data.txt', '12hr')

df.info()

df.sample(10)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
# df = pd.read_csv("your_dataset.csv")  # Replace "your_dataset.csv" with the actual path to your dataset

# Data Preprocessing
# Assuming "user" column contains numerical labels
X = df['message']
y = df['user']

# TF-IDF Vectorization
vectorizer = TfidfVectorizer()
X_vectorized = vectorizer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.4, random_state=90)

# Model Training
model = MultinomialNB()
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Print metrics
print("Accuracy:", accuracy)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
# df = pd.read_csv("your_dataset.csv")  # Replace "your_dataset.csv" with the actual path to your dataset

# Data Preprocessing
X = df['message']
y = df['user']

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)  # Limit the number of features to reduce noise
X_vectorized = vectorizer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

# Model Training
model = LinearSVC()  # Use Support Vector Machine classifier for better performance
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Print metrics
print("Accuracy:", accuracy)

media_messages = df[df['message'] == '<Media omitted>'].shape[0]
print(media_messages)

# pip uninstall emoji

# pip install emoji==1.7.0

import regex
import emoji
import re

def detect_emojis(text):
    # Initialize an empty list to store emojis
    emoji_list = []

    # Define regex pattern to match emojis
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # Emoticons
                               u"\U0001F300-\U0001F5FF"  # Symbols & Pictographs
                               u"\U0001F680-\U0001F6FF"  # Transport & Map Symbols
                               u"\U0001F1E0-\U0001F1FF"  # Flags (iOS)
                               "]+", flags=re.UNICODE)

    # Iterate through each word in the text
    for word in text.split():
        # Find emojis in the word
        emojis = re.findall(emoji_pattern, word)
        # If emojis are found, append them to the emoji_list
        if emojis!=None and emojis!=[''] and emojis!=[]:
            emoji_list.extend(emojis)

    # Return the count of emojis and the emoji_list
    return len(emoji_list), emoji_list

df["emoji"] = df["message"].apply(detect_emojis)
emojis = sum(df['emoji'].str.len())
filtered_df = df[df['emoji'].apply(len) > 1]
# print(df["emoji"].sample(9))
# print("DATA FRAME AFTER ADDED EMOJI COLUMN : ",filtered_df)
URLPATTERN = r'(https?://\S+)'
df['urlcount'] = df.message.apply(lambda x: re.findall(URLPATTERN, x)).str.len()
links = np.sum(df.urlcount)
print("COUNTS : ")
# print("Messages:",total_messages)
print("Messages:",df[df['message'] != ""].shape[0])
print("Media:",media_messages)
print("Emojis:",emojis)
print("Links:",links)

df['day'] = df['date_time'].dt.strftime('%a')
df['month'] = df['date_time'].dt.strftime('%b')
df['year'] = df['date_time'].dt.year
df['date'] = df['date_time'].apply(lambda x: x.date())

df

pip install langdetect

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from langdetect import detect  # Language detection library

# Load the dataset
# data = pd.read_csv("your_dataset.csv")  # Replace "your_dataset.csv" with the path to your dataset

# Preprocessing
# Assuming "message" column contains the text of the messages
documents = df["message"]

# Filter out non-English messages while maintaining index correspondence
english_documents = []
english_indices = []  # To store indices of English messages
for idx, doc in enumerate(documents):
    try:
        if detect(doc) == 'en' and not any(char.isdigit() for char in doc):
            english_documents.append(doc)
            english_indices.append(idx)  # Store index of English message
    except:
        pass  # Ignore errors in language detection

# Feature extraction
vectorizer = CountVectorizer(max_df=0.85, min_df=3, stop_words='english')  # Adjust parameters
X = vectorizer.fit_transform(english_documents)

# Model training (LDA)
num_topics = 6  # You can adjust the number of topics as needed
lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=20, learning_method='online')  # Adjust parameters
lda_model.fit(X)

# Display the topics

import pickle
pickle.dump(lda_model , open('wp-analysis-model.pk1' , 'wb'))

loaded_model = pickle.load(open('wp-analysis-model.pk1' , 'rb'))
print("Topics found via LDA:")
for idx, topic in enumerate(loaded_model.components_):
    print(f"Topic {idx + 1}:")
    topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]
    print([word for word in topic_words if word in vectorizer.get_feature_names_out()])  # Filter meaningful English words

# Optionally, you can assign topics to each document
topic_assignments = loaded_model.transform(X)

# Create a new DataFrame with English messages and topic assignments
english_df = df.loc[english_indices].copy()
english_df['topic'] = topic_assignments.argmax(axis=1)

print(english_df[['message', 'topic']])

from google.colab import drive
drive.mount('/content/drive')

df1 = df.copy()
df1['message_count'] = [1] * df1.shape[0]      # adding extra helper column --> message_count.
df1.drop(columns='year', inplace=True)         # dropping unnecessary columns, using `inplace=True`, since this is copy of the DF and won't affect the original DataFrame.
df1 = df1.groupby('date').sum().reset_index()  # grouping by date; since plot is of frequency of messages --> no. of messages / day.
df1

# Improving Default Styles using Seaborn
sns.set_style("darkgrid")

# For better readablity;
import matplotlib
matplotlib.rcParams['font.size'] = 20
matplotlib.rcParams['figure.figsize'] = (27, 6)
# A basic plot
plt.plot(df1.date, df1.message_count)
plt.title('Messages sent per day over a time period');
# Saving the plots
plt.savefig('msg_plots.svg', format = 'svg')

top10days = df1.sort_values(by="message_count", ascending=False).head(10)    # Sort values according to the number of messages per day.
top10days.reset_index(inplace=True)           # reset index in order.
top10days.drop(columns="index", inplace=True) # dropping original indices.
top10days

# Improving Default Styles using Seaborn
sns.set_style("darkgrid")

# For better readablity;
import matplotlib
matplotlib.rcParams['font.size'] = 10
matplotlib.rcParams['figure.figsize'] = (12, 8)

# A bar plot for top 10 days
sns.barplot(x='date', y='message_count', data=top10days, palette="hls");

# Saving the plots
plt.savefig('top10_days.svg', format = 'svg')

# Total number of people who have sent at least one message on the group;
print(f"Total number of people who have sent at least one message on the group are {len(df.user.unique()) - 1}")   # `-1` because excluding "group_notficiation"

print(f"Number of people who haven't sent even a single message on the group are {237 - len(df.user.unique()) - 1}")

df2 = df.copy()
df2 = df2[df2.user != "group_notification"]
top10df = df2.groupby("user")["message"].count().sort_values(ascending=False)

# Final Data Frame
top10df = top10df.head(10).reset_index()
top10df

def get_colors_of_certain_order(names_in_certain_order):
    '''the color of a certain person remains the same, no matter the plot'''

    order = list(names_in_certain_order)
    return_list = []

    for name in order:
        return_list.append(color_dict[name])

    return return_list

# Adding another column for message length; using the apply method;
df2['message_length'] = df2['message'].apply(lambda x: len(x))

# Creating another dataframe for average length per user;
avg_msg_lengths = df2.groupby(df2.user).mean().reset_index().sort_values(by = 'message_length', ascending = False)

# Creating helper columns;
top10df['avg_message_length'] = [0] * 10
i, j = 0, 0
while i < 10:
    if top10df['user'][i] == avg_msg_lengths['user'][j]:
        top10df['avg_message_length'][i] = avg_msg_lengths['message_length'][j]
        i += 1
        j = -1
    j += 1

# Sorting the average message lengths of the same to 10 active users;
top10df_msg = top10df.sort_values(by = "avg_message_length", ascending=False)
print(top10df_msg)

df3 = df.copy()
df3['message_count'] = [1] * df.shape[0]    # helper column to keep a count.

df3['hour'] = df3['date_time'].apply(lambda x: x.hour)

grouped_by_time = df3.groupby('hour').sum().reset_index().sort_values(by = 'hour')

# specific `order` to be printed in;
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
# grouping by day;
grouped_by_day = df3.groupby('day').sum().reset_index()[['day', 'message_count']]


# specific `order` to be printed in;
months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']   # till Sept, since chats are till Septemeber
# grouping by month;
grouped_by_month = df3.groupby('month').sum().reset_index()[['month', 'message_count']]
print(df3)

comment_words = ' '

# stopwords --> Words to be avoided while forming the WordCloud,
# removed group_notifications like 'joined', 'deleted';
# removed common words
stopwords = STOPWORDS.update(['group', 'link', 'invite', 'joined', 'message', 'deleted', 'yeah', 'hai', 'yes', 'okay', 'ok', 'will', 'use', 'using', 'one', 'know', 'guy', 'group', 'media', 'omitted'])


# iterate through the DataFrame.
for val in df3.message.values:

    # typecaste each val to string.
    val = str(val)

    # split the value.
    tokens = val.split()

    # Converts each token into lowercase.
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    for words in tokens:
        comment_words = comment_words + words + ' '


wordcloud = WordCloud(width = 600, height = 600,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 8).generate(comment_words)

wordcloud.to_image()

#SENTIMENT ANALYSER

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import re
nltk.download('punkt')
nltk.download('opinion_lexicon')

from nltk.tokenize import word_tokenize
from nltk.corpus import opinion_lexicon
# Download NLTK resources (if not downloaded already)
nltk.download('vader_lexicon')

# Get negative words from NLTK opinion lexicon
negative_words = set(opinion_lexicon.negative())
# Function to identify negative words in a text
def identify_negative_words(text):
    tokens = word_tokenize(text.lower())
    negative_word_count = sum(1 for word in tokens if word in negative_words)
    return negative_word_count, [word for word in tokens if word in negative_words]

positive_words = set(opinion_lexicon.positive())

# Function to identify positive words in a text
def identify_positive_words(text):
    tokens = word_tokenize(text.lower())
    positive_word_count = sum(1 for word in tokens if word in positive_words)
    return positive_word_count, [word for word in tokens if word in positive_words]

# Initialize sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Example text
example_text_index = 13654  # Index of the text you want to use from the DataFrame
text = df.loc[example_text_index,'message']
# text="Get out from here. I don't want to meet you"
# Sentiment analysis
sentiment_score = sia.polarity_scores(text)
negative_word_count, negative_words_list = identify_negative_words(text)
positive_word_count, positive_words_list = identify_positive_words(text)
sentiment_label = "Positive" if sentiment_score['compound'] > 0 else "Negative" if sentiment_score['compound'] < 0 else "Neutral"

# Output
print("Sentiment Analysis:")
print("Text:", text)
print("Sentiment Score:", sentiment_score)
print("Sentiment Label:", sentiment_label)
print("\n Negative Words Detected:", negative_words_list)
print("\n Positive Words Detected:", positive_words_list)

'''neg: The negative sentiment score ranging from 0 to 1, where 0 indicates no negative sentiment and 1 indicates the highest negative sentiment.
neu: The neutral sentiment score ranging from 0 to 1, where 0 indicates no neutral sentiment and 1 indicates the highest neutral sentiment.
pos: The positive sentiment score ranging from 0 to 1, where 0 indicates no positive sentiment and 1 indicates the highest positive sentiment.
compound: The compound score is a metric that calculates the overall sentiment. It ranges from -1 (most negative) to 1 (most positive). A compound score above 0 generally indicates a positive sentiment, while a score below 0 indicates a negative sentiment.'''